{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "from scipy.stats.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read in 16 datasets of json from fox\n",
    "twentyfox1 = pd.read_json(\"20thcenturyfox0.json\")\n",
    "\n",
    "twentyfox2 = pd.read_json(\"20thcenturyfox1.json\")\n",
    "twentyfox3 = pd.read_json(\"20thcenturyfox2.json\")\n",
    "twentyfox4 = pd.read_json(\"20thcenturyfox3.json\")\n",
    "twentyfox5 = pd.read_json(\"20thcenturyfox4.json\")\n",
    "twentyfox6 = pd.read_json(\"20thcenturyfox5.json\")\n",
    "twentyfox7 = pd.read_json(\"20thcenturyfox6.json\")\n",
    "twentyfox8 = pd.read_json(\"20thcenturyfox7.json\")\n",
    "twentyfox9 = pd.read_json(\"20thcenturyfox8.json\")\n",
    "twentyfox10 = pd.read_json(\"20thcenturyfox9.json\")\n",
    "twentyfox11 = pd.read_json(\"20thcenturyfox10.json\")\n",
    "twentyfox12 = pd.read_json(\"20thcenturyfox11.json\")\n",
    "twentyfox13 = pd.read_json(\"20thcenturyfox12.json\")\n",
    "twentyfox14 = pd.read_json(\"20thcenturyfox13.json\")\n",
    "twentyfox15 = pd.read_json(\"20thcenturyfox14.json\")\n",
    "twentyfox16 = pd.read_json(\"20thcenturyfox15.json\")\n",
    "fox_map_reduced = pd.concat([twentyfox1,twentyfox2,twentyfox3,twentyfox4,twentyfox5,twentyfox6,twentyfox7,twentyfox8,twentyfox9,twentyfox10,twentyfox11,twentyfox12,twentyfox13,twentyfox14,twentyfox15,twentyfox16], ignore_index=True)\n",
    "#fox_map_reduce = pd.concat([twentyfox1,twentyfox2,twentyfox3], ignore_index=True)\n",
    "\n",
    "fox_map_reduced_text = fox_map_reduced[[\"text\"]]\n",
    "fox_baseline_mr = fox_map_reduced_text\n",
    "#fox_map_reduced_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "alflacduck0 = pd.read_json(\"aflacduck0.json\")\n",
    "alflacduck1 = pd.read_json(\"aflacduck1.json\")\n",
    "alflacduck2 = pd.read_json(\"aflacduck2.json\")\n",
    "alflacduck3 = pd.read_json(\"aflacduck3.json\")\n",
    "alflacduck4 = pd.read_json(\"aflacduck4.json\")\n",
    "alflacduck8 = pd.read_json(\"aflacduck8.json\")\n",
    "\n",
    "alflacduck10 = pd.read_json(\"aflacduck10.json\")\n",
    "alflacduck11 = pd.read_json(\"aflacduck11.json\")\n",
    "alflacduck12 = pd.read_json(\"aflacduck12.json\")\n",
    "alflacduck13 = pd.read_json(\"aflacduck13.json\")\n",
    "alflacduck14 = pd.read_json(\"aflacduck14.json\")\n",
    "\n",
    "alflacduckConcat =  pd.concat([alflacduck0,alflacduck1,alflacduck2,alflacduck3,alflacduck4,alflacduck8,alflacduck10,alflacduck11,alflacduck12,alflacduck13,alflacduck14], ignore_index=True)\n",
    "#alflacduckConcat.head()\n",
    "aflacduck_mr = alflacduckConcat[[\"text\"]]\n",
    "#aflacduck_mr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#aflac\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "alflac0 = pd.read_json(\"aflac0.json\")\n",
    "alflac1 = pd.read_json(\"aflac1.json\")\n",
    "alflac2 = pd.read_json(\"aflac2.json\")\n",
    "alflac3 = pd.read_json(\"aflac3.json\")\n",
    "alflac4 = pd.read_json(\"aflac4.json\")\n",
    "alflac8 = pd.read_json(\"aflac8.json\")\n",
    "\n",
    "alflac10 = pd.read_json(\"aflac10.json\")\n",
    "alflac11 = pd.read_json(\"aflac11.json\")\n",
    "alflac12 = pd.read_json(\"aflac12.json\")\n",
    "alflac13 = pd.read_json(\"aflac13.json\")\n",
    "alflac14 = pd.read_json(\"aflac14.json\")\n",
    "alflac15 = pd.read_json(\"aflac15.json\")\n",
    "alflacConcat =  pd.concat([alflac0,alflac1,alflac2,alflac3,alflac4,alflac8,alflac10,alflac11,alflac12,alflac13,alflac14,alflac15], ignore_index=True)\n",
    "#alflacConcat.head()\n",
    "aflac_mr = alflacConcat[[\"text\"]]\n",
    "#aflac_mr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#advilrelief\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "advilrefer0 = pd.read_json(\"advilrelief0.json\")\n",
    "advilrefer1 = pd.read_json(\"advilrelief1.json\")\n",
    "advilrefer2 = pd.read_json(\"advilrelief2.json\")\n",
    "advilrefer3 = pd.read_json(\"advilrelief3.json\")\n",
    "advilrefer4 = pd.read_json(\"advilrelief4.json\")\n",
    "advilrefer8 = pd.read_json(\"advilrelief8.json\")\n",
    "\n",
    "advilrefer10 = pd.read_json(\"advilrelief10.json\")\n",
    "advilrefer11 = pd.read_json(\"advilrelief11.json\")\n",
    "advilrefer12 = pd.read_json(\"advilrelief12.json\")\n",
    "advilrefer13 = pd.read_json(\"advilrelief13.json\")\n",
    "advilrefer14 = pd.read_json(\"advilrelief14.json\")\n",
    "advilrefer15 = pd.read_json(\"advilrelief15.json\")\n",
    "advilreferConcat =  pd.concat([advilrefer0,advilrefer1,advilrefer2,advilrefer3,advilrefer4,advilrefer8,advilrefer10,advilrefer11,advilrefer12,advilrefer13,advilrefer14,advilrefer15], ignore_index=True)\n",
    "#advilreferConcat.head()\n",
    "advilrelief_mr = advilreferConcat[[\"text\"]]\n",
    "#advilrelief_mr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#abcnetwork, lots of missing data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "abcdnetwork0 = pd.read_json(\"abcnetwork0.json\")\n",
    "abcdnetwork2 = pd.read_json(\"abcnetwork2.json\")\n",
    "abcdnetwork3 = pd.read_json(\"abcnetwork3.json\")\n",
    "abcdnetwork4 = pd.read_json(\"abcnetwork4.json\")\n",
    "abcdnetwork10 = pd.read_json(\"abcnetwork10.json\")\n",
    "abcdnetwork11 = pd.read_json(\"abcnetwork11.json\")\n",
    "abcdnetwork12 = pd.read_json(\"abcnetwork12.json\")\n",
    "abcdnetwork13 = pd.read_json(\"abcnetwork13.json\")\n",
    "abcdnetwork14 = pd.read_json(\"abcnetwork14.json\")\n",
    "abcdnetwork15 = pd.read_json(\"abcnetwork15.json\")\n",
    "abcdnetworkConcat =  pd.concat([abcdnetwork0,abcdnetwork2,abcdnetwork3,abcdnetwork4,abcdnetwork10,abcdnetwork11,abcdnetwork12,abcdnetwork13,abcdnetwork14,abcdnetwork15], ignore_index=True)\n",
    "#abcdnetworkConcat.head()\n",
    "abcnetwork_mr = abcdnetworkConcat[[\"text\"]]\n",
    "#abcnetwork_mr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#abcworldnews\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "abcdworldnews0 = pd.read_json(\"abcworldnews0.json\")\n",
    "abcdworldnews1 = pd.read_json(\"abcworldnews1.json\")\n",
    "abcdworldnews2 = pd.read_json(\"abcworldnews2.json\")\n",
    "abcdworldnews3 = pd.read_json(\"abcworldnews3.json\")\n",
    "abcdworldnews4 = pd.read_json(\"abcworldnews4.json\")\n",
    "abcdworldnews10 = pd.read_json(\"abcworldnews10.json\")\n",
    "abcdworldnews11 = pd.read_json(\"abcworldnews11.json\")\n",
    "abcdworldnews12 = pd.read_json(\"abcworldnews12.json\")\n",
    "abcdworldnews13 = pd.read_json(\"abcworldnews13.json\")\n",
    "abcdworldnews14 = pd.read_json(\"abcworldnews14.json\")\n",
    "abcdworldnews15 = pd.read_json(\"abcworldnews15.json\")\n",
    "abcdworldnewsConcat =  pd.concat([abcdworldnews0,abcdworldnews1,abcdworldnews2,abcdworldnews3,abcdworldnews4,abcdworldnews10,abcdworldnews11,abcdworldnews12,abcdworldnews13,abcdworldnews14,abcdworldnews15], ignore_index=True)\n",
    "#abcdworldnewsConcat.head()\n",
    "abcworldnews_mr = abcdworldnewsConcat[[\"text\"]]\n",
    "#abcworldnews_mr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#acura\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "acuray0 = pd.read_json(\"acura0.json\")\n",
    "acuray1 = pd.read_json(\"acura1.json\")\n",
    "acuray2 = pd.read_json(\"acura2.json\")\n",
    "acuray3 = pd.read_json(\"acura3.json\")\n",
    "acuray4 = pd.read_json(\"acura4.json\")\n",
    "acuray8 = pd.read_json(\"acura8.json\")\n",
    "acuray10 = pd.read_json(\"acura10.json\")\n",
    "acuray11 = pd.read_json(\"acura11.json\")\n",
    "acuray12 = pd.read_json(\"acura12.json\")\n",
    "acuray13 = pd.read_json(\"acura13.json\")\n",
    "acuray14 = pd.read_json(\"acura14.json\")\n",
    "acuray15 = pd.read_json(\"acura15.json\")\n",
    "acurayConcat =  pd.concat([acuray0,acuray1,acuray2,acuray3,acuray4,acuray8,acuray10,acuray11,acuray12,acuray13,acuray14,acuray15], ignore_index=True)\n",
    "#acurayConcat.head()\n",
    "acura_mr = acurayConcat[[\"text\"]]\n",
    "#acura_mr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#advilrelief\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "advilrefer0 = pd.read_json(\"advilrelief0.json\")\n",
    "advilrefer1 = pd.read_json(\"advilrelief1.json\")\n",
    "advilrefer2 = pd.read_json(\"advilrelief2.json\")\n",
    "advilrefer3 = pd.read_json(\"advilrelief3.json\")\n",
    "advilrefer4 = pd.read_json(\"advilrelief4.json\")\n",
    "advilrefer8 = pd.read_json(\"advilrelief8.json\")\n",
    "\n",
    "advilrefer10 = pd.read_json(\"advilrelief10.json\")\n",
    "advilrefer11 = pd.read_json(\"advilrelief11.json\")\n",
    "advilrefer12 = pd.read_json(\"advilrelief12.json\")\n",
    "advilrefer13 = pd.read_json(\"advilrelief13.json\")\n",
    "advilrefer14 = pd.read_json(\"advilrelief14.json\")\n",
    "advilrefer15 = pd.read_json(\"advilrelief15.json\")\n",
    "advilreferConcat =  pd.concat([advilrefer0,advilrefer1,advilrefer2,advilrefer3,advilrefer4,advilrefer8,advilrefer10,advilrefer11,advilrefer12,advilrefer13,advilrefer14,advilrefer15], ignore_index=True)\n",
    "#advilreferConcat.head()\n",
    "advilrelief_mr = advilreferConcat[[\"text\"]]\n",
    "#advilrelief_mr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#aflacwallstreet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "aflacwallstreets0 = pd.read_json(\"aflacwallstreet0.json\")\n",
    "aflacwallstreets1 = pd.read_json(\"aflacwallstreet1.json\")\n",
    "aflacwallstreets2 = pd.read_json(\"aflacwallstreet2.json\")\n",
    "aflacwallstreets3 = pd.read_json(\"aflacwallstreet3.json\")\n",
    "aflacwallstreets4 = pd.read_json(\"aflacwallstreet4.json\")\n",
    "aflacwallstreets8 = pd.read_json(\"aflacwallstreet8.json\")\n",
    "\n",
    "aflacwallstreets10 = pd.read_json(\"aflacwallstreet10.json\")\n",
    "aflacwallstreets11 = pd.read_json(\"aflacwallstreet11.json\")\n",
    "aflacwallstreets12 = pd.read_json(\"aflacwallstreet12.json\")\n",
    "aflacwallstreets13 = pd.read_json(\"aflacwallstreet13.json\")\n",
    "aflacwallstreets14 = pd.read_json(\"aflacwallstreet14.json\")\n",
    "aflacwallstreets15 = pd.read_json(\"aflacwallstreet15.json\")\n",
    "aflacwallstreetsConcat =  pd.concat([aflacwallstreets0,aflacwallstreets1,aflacwallstreets2,aflacwallstreets3,aflacwallstreets4,aflacwallstreets8,aflacwallstreets10,aflacwallstreets11,aflacwallstreets12,aflacwallstreets13,aflacwallstreets14,aflacwallstreets15], ignore_index=True)\n",
    "#aflacwallstreetsConcat.head()\n",
    "aflacwallstreet_mr = aflacwallstreetsConcat[[\"text\"]]\n",
    "#aflacwallstreet_mr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#allstate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "allstates0 = pd.read_json(\"allstate0.json\")\n",
    "allstates1 = pd.read_json(\"allstate1.json\")\n",
    "allstates2 = pd.read_json(\"allstate2.json\")\n",
    "allstates3 = pd.read_json(\"allstate3.json\")\n",
    "allstates4 = pd.read_json(\"allstate4.json\")\n",
    "allstates8 = pd.read_json(\"allstate8.json\")\n",
    "allstates10 = pd.read_json(\"allstate10.json\")\n",
    "allstates11 = pd.read_json(\"allstate11.json\")\n",
    "allstates12 = pd.read_json(\"allstate12.json\")\n",
    "allstates13 = pd.read_json(\"allstate13.json\")\n",
    "allstates14 = pd.read_json(\"allstate14.json\")\n",
    "allstates15 = pd.read_json(\"allstate15.json\")\n",
    "allstatesConcat =  pd.concat([allstates0,allstates1,allstates2,allstates3,allstates4,allstates8,allstates10,allstates11,allstates12,allstates13,allstates14,allstates15], ignore_index=True)\n",
    "#allstatesConcat.head()\n",
    "allstate_mr = allstatesConcat[[\"text\"]]\n",
    "#allstate_mr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wendy0 = pd.read_json(\"Wendys0.json\")\n",
    "wendy1 = pd.read_json(\"Wendys1.json\")\n",
    "wendy2 = pd.read_json(\"Wendys2.json\")\n",
    "wendy3 = pd.read_json(\"Wendys3.json\")\n",
    "wendy4 = pd.read_json(\"Wendys4.json\")\n",
    "wendy5 = pd.read_json(\"Wendys5.json\")\n",
    "wendy6 = pd.read_json(\"Wendys6.json\")\n",
    "wendy7 = pd.read_json(\"Wendys7.json\")\n",
    "wendy8 = pd.read_json(\"Wendys8.json\")\n",
    "wendy9 = pd.read_json(\"Wendys9.json\")\n",
    "wendy10 = pd.read_json(\"Wendys10.json\")\n",
    "wendy11 = pd.read_json(\"Wendys11.json\")\n",
    "wendy12 = pd.read_json(\"Wendys12.json\")\n",
    "wendy13 = pd.read_json(\"Wendys13.json\")\n",
    "wendy14 = pd.read_json(\"Wendys14.json\")\n",
    "wendy15 = pd.read_json(\"Wendys15.json\")\n",
    "concatWendy = pd.concat([wendy0, wendy1, wendy2, wendy3, wendy4, wendy5, wendy6, wendy7, wendy8, wendy9, wendy10, wendy11, wendy12, wendy13, wendy14, wendy15])\n",
    "wendy_mr = concatWendy[['text']]\n",
    "#mapreduce_wendy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#amazon\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "amazony0 = pd.read_json(\"amazon0.json\")\n",
    "amazony1 = pd.read_json(\"amazon1.json\")\n",
    "amazony2 = pd.read_json(\"amazon2.json\")\n",
    "amazony3 = pd.read_json(\"amazon3.json\")\n",
    "amazony4 = pd.read_json(\"amazon4.json\")\n",
    "amazony8 = pd.read_json(\"amazon8.json\")\n",
    "\n",
    "amazony10 = pd.read_json(\"amazon10.json\")\n",
    "amazony11 = pd.read_json(\"amazon11.json\")\n",
    "amazony12 = pd.read_json(\"amazon12.json\")\n",
    "amazony13 = pd.read_json(\"amazon13.json\")\n",
    "amazony14 = pd.read_json(\"amazon14.json\")\n",
    "amazony15 = pd.read_json(\"amazon15.json\")\n",
    "amazonyConcat =  pd.concat([amazony0,amazony1,amazony2,amazony3,amazony4,amazony8,amazony10,amazony11,amazony12,amazony13,amazony14,amazony15], ignore_index=True)\n",
    "#amazonyConcat.head()\n",
    "amazon_mr = amazonyConcat[[\"text\"]]\n",
    "#amazon_mr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#amazon_grocery\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "amazon_groceryy_grocery0 = pd.read_json(\"amazon_grocery0.json\")\n",
    "amazon_groceryy_grocery1 = pd.read_json(\"amazon_grocery1.json\")\n",
    "amazon_groceryy_grocery2 = pd.read_json(\"amazon_grocery2.json\")\n",
    "amazon_groceryy_grocery3 = pd.read_json(\"amazon_grocery3.json\")\n",
    "amazon_groceryy_grocery4 = pd.read_json(\"amazon_grocery4.json\")\n",
    "amazon_groceryy_grocery8 = pd.read_json(\"amazon_grocery8.json\")\n",
    "\n",
    "amazon_groceryy_grocery10 = pd.read_json(\"amazon_grocery10.json\")\n",
    "amazon_groceryy_grocery11 = pd.read_json(\"amazon_grocery11.json\")\n",
    "amazon_groceryy_grocery12 = pd.read_json(\"amazon_grocery12.json\")\n",
    "amazon_groceryy_grocery13 = pd.read_json(\"amazon_grocery13.json\")\n",
    "amazon_groceryy_grocery14 = pd.read_json(\"amazon_grocery14.json\")\n",
    "amazon_groceryy_grocery15 = pd.read_json(\"amazon_grocery15.json\")\n",
    "amazon_groceryy_groceryConcat =  pd.concat([amazon_groceryy_grocery0,amazon_groceryy_grocery1,amazon_groceryy_grocery2,amazon_groceryy_grocery3,amazon_groceryy_grocery4,amazon_groceryy_grocery8,amazon_groceryy_grocery10,amazon_groceryy_grocery11,amazon_groceryy_grocery12,amazon_groceryy_grocery13,amazon_groceryy_grocery14,amazon_groceryy_grocery15], ignore_index=True)\n",
    "#amazon_groceryy_groceryConcat.head()\n",
    "amazon_grocery_mr = amazon_groceryy_groceryConcat[[\"text\"]]\n",
    "#amazon_grocery_mr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#amazonbooks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "amazonbooksybooks0 = pd.read_json(\"amazonbooks0.json\")\n",
    "amazonbooksybooks1 = pd.read_json(\"amazonbooks1.json\")\n",
    "amazonbooksybooks2 = pd.read_json(\"amazonbooks2.json\")\n",
    "amazonbooksybooks3 = pd.read_json(\"amazonbooks3.json\")\n",
    "amazonbooksybooks4 = pd.read_json(\"amazonbooks4.json\")\n",
    "amazonbooksybooks8 = pd.read_json(\"amazonbooks8.json\")\n",
    "\n",
    "amazonbooksybooks10 = pd.read_json(\"amazonbooks10.json\")\n",
    "amazonbooksybooks11 = pd.read_json(\"amazonbooks11.json\")\n",
    "amazonbooksybooks12 = pd.read_json(\"amazonbooks12.json\")\n",
    "amazonbooksybooks13 = pd.read_json(\"amazonbooks13.json\")\n",
    "amazonbooksybooks14 = pd.read_json(\"amazonbooks14.json\")\n",
    "amazonbooksybooks15 = pd.read_json(\"amazonbooks15.json\")\n",
    "amazonbooksybooksConcat =  pd.concat([amazonbooksybooks0,amazonbooksybooks1,amazonbooksybooks2,amazonbooksybooks3,amazonbooksybooks4,amazonbooksybooks8,amazonbooksybooks10,amazonbooksybooks11,amazonbooksybooks12,amazonbooksybooks13,amazonbooksybooks14,amazonbooksybooks15], ignore_index=True)\n",
    "#amazonbooksybooksConcat.head()\n",
    "amazonbooks_mr = amazonbooksybooksConcat[[\"text\"]]\n",
    "#amazonbooks_mr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Chase\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "Chaser0 = pd.read_json(\"Chase0.json\")\n",
    "Chaser1 = pd.read_json(\"Chase1.json\")\n",
    "Chaser2 = pd.read_json(\"Chase2.json\")\n",
    "Chaser3 = pd.read_json(\"Chase3.json\")\n",
    "Chaser4 = pd.read_json(\"Chase4.json\")\n",
    "Chaser8 = pd.read_json(\"Chase8.json\")\n",
    "Chaser10 = pd.read_json(\"Chase10.json\")\n",
    "Chaser11 = pd.read_json(\"Chase11.json\")\n",
    "Chaser12 = pd.read_json(\"Chase12.json\")\n",
    "Chaser13 = pd.read_json(\"Chase13.json\")\n",
    "Chaser14 = pd.read_json(\"Chase14.json\")\n",
    "Chaser15 = pd.read_json(\"Chase15.json\")\n",
    "ChaserConcat =  pd.concat([Chaser0,Chaser1,Chaser2,Chaser3,Chaser4,Chaser8,Chaser10,Chaser11,Chaser12,Chaser13,Chaser14,Chaser15], ignore_index=True)\n",
    "#ChaserConcat.head()\n",
    "Chase_mr = ChaserConcat[[\"text\"]]\n",
    "#Chase_mr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#chilis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "chiliss0 = pd.read_json(\"chilis0.json\")\n",
    "chiliss1 = pd.read_json(\"chilis1.json\")\n",
    "chiliss2 = pd.read_json(\"chilis2.json\")\n",
    "chiliss3 = pd.read_json(\"chilis3.json\")\n",
    "chiliss4 = pd.read_json(\"chilis4.json\")\n",
    "chiliss8 = pd.read_json(\"chilis8.json\")\n",
    "chiliss10 = pd.read_json(\"chilis10.json\")\n",
    "chiliss11 = pd.read_json(\"chilis11.json\")\n",
    "chiliss12 = pd.read_json(\"chilis12.json\")\n",
    "chiliss13 = pd.read_json(\"chilis13.json\")\n",
    "chiliss14 = pd.read_json(\"chilis14.json\")\n",
    "chiliss15 = pd.read_json(\"chilis15.json\")\n",
    "chilissConcat =  pd.concat([chiliss0,chiliss1,chiliss2,chiliss3,chiliss4,chiliss8,chiliss10,chiliss11,chiliss12,chiliss13,chiliss14,chiliss15], ignore_index=True)\n",
    "#chilissConcat.head()\n",
    "chilis_mr = chilissConcat[[\"text\"]]\n",
    "#chilis_mr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#citi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "citti0 = pd.read_json(\"citi0.json\")\n",
    "citti1 = pd.read_json(\"citi1.json\")\n",
    "citti2 = pd.read_json(\"citi2.json\")\n",
    "citti3 = pd.read_json(\"citi3.json\")\n",
    "citti4 = pd.read_json(\"citi4.json\")\n",
    "citti8 = pd.read_json(\"citi8.json\")\n",
    "citti10 = pd.read_json(\"citi10.json\")\n",
    "citti11 = pd.read_json(\"citi11.json\")\n",
    "citti12 = pd.read_json(\"citi12.json\")\n",
    "citti13 = pd.read_json(\"citi13.json\")\n",
    "citti14 = pd.read_json(\"citi14.json\")\n",
    "citti15 = pd.read_json(\"citi15.json\")\n",
    "cittiConcat =  pd.concat([citti0,citti1,citti2,citti3,citti4,citti8,citti10,citti11,citti12,citti13,citti14,citti15], ignore_index=True)\n",
    "#cittiConcat.head()\n",
    "citi_mr = cittiConcat[[\"text\"]]\n",
    "#citi_mr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#citibank\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "cittibank0 = pd.read_json(\"citibank0.json\")\n",
    "cittibank1 = pd.read_json(\"citibank1.json\")\n",
    "cittibank2 = pd.read_json(\"citibank2.json\")\n",
    "cittibank3 = pd.read_json(\"citibank3.json\")\n",
    "cittibank4 = pd.read_json(\"citibank4.json\")\n",
    "cittibank8 = pd.read_json(\"citibank8.json\")\n",
    "cittibank10 = pd.read_json(\"citibank10.json\")\n",
    "cittibank11 = pd.read_json(\"citibank11.json\")\n",
    "cittibank12 = pd.read_json(\"citibank12.json\")\n",
    "cittibank13 = pd.read_json(\"citibank13.json\")\n",
    "cittibank14 = pd.read_json(\"citibank14.json\")\n",
    "cittibank15 = pd.read_json(\"citibank15.json\")\n",
    "cittibankConcat =  pd.concat([cittibank0,cittibank1,cittibank2,cittibank3,cittibank4,cittibank8,cittibank10,cittibank11,cittibank12,cittibank13,cittibank14,cittibank15], ignore_index=True)\n",
    "#cittibankConcat.head()\n",
    "citibank_mr = cittibankConcat[[\"text\"]]\n",
    "#citibank_mr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "insults = pd.read_csv(\"insults.csv\")\n",
    "insults.head()\n",
    "#insults.dtypes\n",
    "\n",
    "insultsmapreducce = insults[[\"Comment\", \"Insult\"]]\n",
    "df = insultsmapreducce.rename(columns={'Comment': 'text', 'Insult': 'id'})\n",
    "df = df.drop(df[df.id ==0].index)\n",
    "#remove all neutral comments\n",
    "newdf = df[['text']]\n",
    "#why it didnt work last time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30985, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anti = pd.concat([newdf,citibank_mr,citi_mr,amazonbooks_mr,amazon_grocery_mr,amazon_mr,wendy_mr,chilis_mr,Chase_mr,advilrelief_mr,acura_mr,allstate_mr,aflacwallstreet_mr,abcworldnews_mr,abcnetwork_mr,aflac_mr,aflacduck_mr])\n",
    "#anti.head()\n",
    "anti.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'foxyclass.txt'\n",
    "fox_map_reduced_text.to_csv(filename, index=False, header=False, encoding='utf-8')\n",
    "\n",
    "filename1 = 'anti.txt'\n",
    "anti.to_csv(filename1, index=False, header=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_pos = []\n",
    "labels_pos = []\n",
    "with open(\"foxyclass.txt\", encoding=\"utf8\") as f:\n",
    "    for i in f: \n",
    "        text_pos.append(i) \n",
    "        labels_pos.append('pos')\n",
    "\n",
    "text_neg = []\n",
    "labels_neg = []\n",
    "with open(\"anti.txt\",encoding=\"utf8\") as f:\n",
    "    for i in f: \n",
    "        text_neg.append(i)\n",
    "        labels_neg.append('neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "training_text = text_pos[:int((.8)*len(text_pos))] + text_neg[:int((.8)*len(text_neg))]\n",
    "training_labels = labels_pos[:int((.8)*len(labels_pos))] + labels_neg[:int((.8)*len(labels_neg))]\n",
    "\n",
    "test_text = text_pos[int((.8)*len(text_pos)):] + text_neg[int((.8)*len(text_neg)):]\n",
    "test_labels = labels_pos[int((.8)*len(labels_pos)):] + labels_neg[int((.8)*len(labels_neg)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    lowercase = False,\n",
    "    max_features = 85\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = vectorizer.fit_transform(\n",
    "    training_text + test_text)\n",
    "\n",
    "features_nd = features.toarray() # for easy use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Test\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(\n",
    "        features_nd[0:len(training_text)], \n",
    "        training_labels,\n",
    "        train_size=0.80, \n",
    "        random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_model = LogisticRegression()\n",
    "log_model = log_model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = log_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.913736358912\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "def format_sentence(sent):\n",
    "    return({word: True for word in nltk.word_tokenize(sent)})\n",
    "pos = []\n",
    "with open(\"foxyclass.txt\", encoding= 'utf8') as f:\n",
    "    for i in f: \n",
    "        pos.append([format_sentence(i), 'pos'])\n",
    "        \n",
    "neg = []\n",
    "with open(\"anti.txt\", encoding= 'utf8') as f:\n",
    "    for i in f: \n",
    "        neg.append([format_sentence(i), 'neg'])\n",
    "\n",
    "training = pos[:int((.8)*len(pos))] + neg[:int((.8)*len(neg))]\n",
    "test = pos[int((.8)*len(pos)):] + neg[int((.8)*len(neg)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "           FantasticFour = True              pos : neg    =    640.8 : 1.0\n",
      "                theaters = True              pos : neg    =    613.2 : 1.0\n",
      "      UnfinishedBusiness = True              pos : neg    =    354.1 : 1.0\n",
      "              Apocalypse = True              pos : neg    =    318.9 : 1.0\n",
      "         PeregrinesMovie = True              pos : neg    =    301.4 : 1.0\n",
      "                Deadpool = True              pos : neg    =    275.6 : 1.0\n",
      "          20thcenturyfox = True              pos : neg    =    272.1 : 1.0\n",
      "                Premiere = True              pos : neg    =    231.2 : 1.0\n",
      "               OscarNoms = True              pos : neg    =    201.9 : 1.0\n",
      "                    SDCC = True              pos : neg    =    184.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(training)\n",
    "\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.810559866962306\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.util import accuracy\n",
    "print(accuracy(classifier, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df.to_pickle(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
